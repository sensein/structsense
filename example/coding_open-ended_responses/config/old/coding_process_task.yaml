

agreement_task:
  description: >
    You are given the {coder_1_categories} and {coder_2_categories} from two different coding agents.
    Compare the outputs from Coder1 and Coder2 to compute inter-coder reliability.
    After comparing assigned categories for all responses, calculate percent agreement and Cohenâ€™s Kappa.
    List all response IDs where the two coders disagreed (i.e., sets of assigned codes do not match exactly).
  expected_output: >
    output format: json
    Example output:
    {
      "percent_agreement": 0.75,
      "cohens_kappa": 0.6,
      "disagreements": [
        {
          "response_id": 4,
          "coder_1_category": "",
          "coder_2_category": "Data Literacy Challenges",
          "reason": "Coder 1 did not assign a category, while Coder 2 assigned it to Data Literacy Challenges."
        }
      ]
    }
  agent_id: agreement_agent

disagreement_resolution_task:
  description: >
    You are given the {disagreements} from the agreement task.
    For each disagreement, provide a resolution by selecting one of the following options:
    1. Assign the response to Coder 1's category.
    2. Assign the response to Coder 2's category.
    3. Create a new category that aligns more semantically with the response.
    4. Leave the response unclassified (no category assigned).
    5. Provide a brief explanation for your choice.
  expected_output: >
    output format: json
    Example output:
    {
      "resolved_disagreements": [
        {
          "response_id": 4,
          "assigned_category": "Data Literacy Challenges",
          "reason": "Coder 2's category aligns better with the response content."
        }
      ]
    }
  agent_id: disagreement_resolution_agent

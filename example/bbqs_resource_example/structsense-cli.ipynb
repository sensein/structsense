{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089cf773-5a8e-47e8-8fb8-04054b195cb2",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a637454f-04d0-44cc-a1c4-7235bee34310",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install structsense --quiet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c64a6973-fbbc-479f-bf5c-85725f94541f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: structsense-cli [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  CLI commands for the Structsense Framework application\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  extract  Extract the terms along with sentence.\n"
     ]
    }
   ],
   "source": [
    "!structsense-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6801df16-eab8-4c36-91fa-3a69010f4e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: structsense-cli extract [OPTIONS]\n",
      "\n",
      "  Extract the terms along with sentence.\n",
      "\n",
      "Options:\n",
      "  --agentconfig TEXT      Path to the agent configuration in YAML file format\n",
      "                          or dictionary  [required]\n",
      "  --taskconfig TEXT       Path to the agent task configuration in YAML format\n",
      "                          or or dictionary  [required]\n",
      "  --embedderconfig TEXT   Path to the embedding configuration in YAML format\n",
      "                          or or dictionary  [required]\n",
      "  --flowconfig TEXT       Path to the flow configuration in YAML format or or\n",
      "                          dictionary. The flow configuration describes the\n",
      "                          flow of the agent.  [required]\n",
      "  --knowledgeconfig TEXT  Path to the configuration in YAML format or or\n",
      "                          dictionary that specify the search knowledge search\n",
      "                          key.\n",
      "  --source TEXT           The source—whether a file (text or PDF), a folder,\n",
      "                          or a text string.  [required]\n",
      "  --help                  Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!structsense-cli extract --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee709161-3acd-4536-b9b9-2833c92379ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mbbqs_resource_config\u001b[m\u001b[m  \u001b[34mcrew_memory\u001b[m\u001b[m           structsense-cli.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4effa86e-bba4-4b5d-9175-a96acb1153ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbqs_agent.yaml                flow_bbqs.yaml\n",
      "bbqs_task.yaml                 search_ontology_knowledge.yaml\n",
      "embedding.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls bbqs_resource_config/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3869eaa7-11e5-4c02-88e5-e3ada431eca0",
   "metadata": {},
   "source": [
    "# 1. Extract from PDF (minimal setup without knowledge source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1021c957",
   "metadata": {},
   "source": [
    "PDF: https://www.nature.com/articles/s41593-018-0209-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be01a220-3317-4a3c-9d82-0efc11c1543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 14:05:32,879 - structsense.cli - INFO - Processing source: /Users/brukewossenseged/Documents/dlc.pdf with agent config: bbqs_resource_config/bbqs_agent.yaml, task config: bbqs_resource_config/bbqs_task.yaml, embedderconfig: bbqs_resource_config/embedding.yaml, flowconfig: bbqs_resource_config/flow_bbqs.yaml, knowledgeconfig: None\n",
      "Processing source: /Users/brukewossenseged/Documents/dlc.pdf with agent config: bbqs_resource_config/bbqs_agent.yaml, task config: bbqs_resource_config/bbqs_task.yaml, embedderconfig: bbqs_resource_config/embedding.yaml knowledgeconfig: None flowconfig: bbqs_resource_config/flow_bbqs.yaml\n",
      "2025-04-04 14:05:32,880 - utils.utils - INFO - Trying paths: ['/Users/brukewossenseged/Documents/dlc.pdf', '/Users/brukewossenseged/Documents/dlc.pdf', '/Users/brukewossenseged/Documents/dlc.pdf', '/Users/brukewossenseged/Documents/dlc.pdf']\n",
      "2025-04-04 14:05:32,880 - utils.utils - INFO - Using path: /Users/brukewossenseged/Documents/dlc.pdf\n",
      "2025-04-04 14:05:32,880 - utils.utils - INFO - Processing single file: /Users/brukewossenseged/Documents/dlc.pdf\n",
      "2025-04-04 14:06:03,867 - utils.utils - INFO - Successfully extracted 6 sections\n",
      "\u001b[34m╭─\u001b[0m\u001b[34m──────────────────────────────\u001b[0m\u001b[34m Flow Execution \u001b[0m\u001b[34m──────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
      "\u001b[34m│\u001b[0m                                                                              \u001b[34m│\u001b[0m\n",
      "\u001b[34m│\u001b[0m  \u001b[1;34mStarting Flow Execution\u001b[0m                                                     \u001b[34m│\u001b[0m\n",
      "\u001b[34m│\u001b[0m  \u001b[37mName: \u001b[0m\u001b[34mStructSenseFlow\u001b[0m                                                       \u001b[34m│\u001b[0m\n",
      "\u001b[34m│\u001b[0m  \u001b[37mID: \u001b[0m\u001b[34m1122876d-91d0-4213-9bad-6660a729cf99\u001b[0m                                    \u001b[34m│\u001b[0m\n",
      "\u001b[34m│\u001b[0m                                                                              \u001b[34m│\u001b[0m\n",
      "\u001b[34m│\u001b[0m                                                                              \u001b[34m│\u001b[0m\n",
      "\u001b[34m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\n",
      "2025-04-04 14:06:03,882 - utils.utils - INFO - Trying config paths: ['bbqs_resource_config/bbqs_agent.yaml', '/Users/brukewossenseged/structsense/example/bbqs_resource_example/bbqs_resource_config/bbqs_agent.yaml', '/Users/brukewossenseged/structsense/example/bbqs_resource_example/bbqs_resource_config/bbqs_agent.yaml', '/Users/brukewossenseged/structsense/example/bbqs_resource_example/bbqs_resource_config/bbqs_agent.yaml']\n",
      "2025-04-04 14:06:03,902 - utils.utils - INFO - file processing - <_io.TextIOWrapper name='bbqs_resource_config/bbqs_agent.yaml' mode='r' encoding='utf-8'>, type: agent\n",
      "2025-04-04 14:06:03,903 - utils.utils - INFO - Trying config paths: ['bbqs_resource_config/bbqs_task.yaml', '/Users/brukewossenseged/structsense/example/bbqs_resource_example/bbqs_resource_config/bbqs_task.yaml', '/Users/brukewossenseged/structsense/example/bbqs_resource_example/bbqs_resource_config/bbqs_task.yaml', '/Users/brukewossenseged/structsense/example/bbqs_resource_example/bbqs_resource_config/bbqs_task.yaml']\n",
      "2025-04-04 14:06:03,916 - utils.utils - INFO - file processing - <_io.TextIOWrapper name='bbqs_resource_config/bbqs_task.yaml' mode='r' encoding='utf-8'>, type: task\n",
      "2025-04-04 14:06:03,917 - utils.utils - INFO - Trying config paths: ['bbqs_resource_config/embedding.yaml', '/Users/brukewossenseged/structsense/example/bbqs_resource_example/bbqs_resource_config/embedding.yaml', '/Users/brukewossenseged/structsense/example/bbqs_resource_example/bbqs_resource_config/embedding.yaml', '/Users/brukewossenseged/structsense/example/bbqs_resource_example/bbqs_resource_config/embedding.yaml']\n",
      "2025-04-04 14:06:03,919 - utils.utils - INFO - file processing - <_io.TextIOWrapper name='bbqs_resource_config/embedding.yaml' mode='r' encoding='utf-8'>, type: embedder\n",
      "2025-04-04 14:06:03,920 - utils.utils - INFO - Trying config paths: ['bbqs_resource_config/flow_bbqs.yaml', '/Users/brukewossenseged/structsense/example/bbqs_resource_example/bbqs_resource_config/flow_bbqs.yaml', '/Users/brukewossenseged/structsense/example/bbqs_resource_example/bbqs_resource_config/flow_bbqs.yaml', '/Users/brukewossenseged/structsense/example/bbqs_resource_example/bbqs_resource_config/flow_bbqs.yaml']\n",
      "2025-04-04 14:06:03,926 - utils.utils - INFO - file processing - <_io.TextIOWrapper name='bbqs_resource_config/flow_bbqs.yaml' mode='r' encoding='utf-8'>, type: flow\n",
      "2025-04-04 14:06:04,078 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "\u001b[1;34m🌊 Flow: \u001b[0m\u001b[34mStructSenseFlow\u001b[0m\n",
      "\u001b[37m    ID: \u001b[0m\u001b[34m1122876d-91d0-4213-9bad-6660a729cf99\u001b[0m\n",
      "└── \u001b[33m🧠 Starting Flow...\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[35m Flow started with ID: 1122876d-91d0-4213-9bad-6660a729cf99\u001b[00m\n",
      "2025-04-04 14:06:04,560 - crewai.flow.flow - INFO - Flow started with ID: 1122876d-91d0-4213-9bad-6660a729cf99\n",
      "\u001b[1;34m🌊 Flow: \u001b[0m\u001b[34mStructSenseFlow\u001b[0m\n",
      "\u001b[37m    ID: \u001b[0m\u001b[34m1122876d-91d0-4213-9bad-6660a729cf99\u001b[0m\n",
      "├── \u001b[33m🧠 Starting Flow...\u001b[0m\n",
      "└── \u001b[1;33m🔄 Running:\u001b[0m\u001b[1;33m kickoff_flow\u001b[0m\n",
      "\n",
      "2025-04-04 14:06:04,563 - structsense.app - INFO - Running step: extracted_structured_information\n",
      "2025-04-04 14:06:04,586 - root - ERROR - Error during short_term search: [Errno 61] Connection refused in query.\n",
      "2025-04-04 14:06:04,588 - root - ERROR - Error during short_term search: [Errno 61] Connection refused in query.\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mBrain and Behavior Quantification and Synchronization (BBQS) Resource Extractor Agent\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mExtract structured metadata about scientific resources relevant to BBQS from the given {'metadata': {'title': '', 'authors': ['Mackenzie Weygandt Mathis'], 'abstract': '', 'publication_date': 'April 2018'}, 'sections': [{'heading': 'Untitled Section 1', 'content': \"['A ccurate quantification of behavior is essential for understanding the brain 123 . Both within and beyond the field of neuroscience, there is a fruitful tradition of using cuttingedge technology to study movement. Often, the application of new technology has the potential to reveal unforeseen features of the phenomena being studied, as in the case of Muybridges famous photography studies in the mid-19th century or modern highspeed videography that has revealed previously unknown motor sequences, such as tap dancing in the songbird 2,4,5 . Historically, collected data were analyzed manually, which is a time-consuming, labor-intensive and error-prone process that is prohibitively inefficient at todays high rates of data acquisition. Conversely, advances in computer vision have consistently inspired methods of data analysis to reduce human labor 678 .', 'We are particularly interested in extracting the pose of animals-i.e., the geometrical configuration of multiple body parts. The gold standard for pose estimation in the field of motor control is the combination of video recordings with easily recognizable reflective markers applied to locations of interest, which greatly simplifies subsequent analysis and allows tracking of body parts with high accuracy 9101112 . However, such systems can be expensive and potentially distracting to animals 13,14 , and markers need to be placed before recording, which predefines the features that can be tracked. This mitigates one of the benefits of video data: its low level of invasiveness. One alternative to physical markers is to fit skeleton or active contour models 1314151617 . These methods can work quite well and are fast, but require sophisticated skeleton models, which are difficult to develop and to fit to data, limiting the flexibility of such methods 18,19 . Another alternative is training regressors based on various computationally derived features to track particular body parts in a supervised way 6,13,202122 . Training predictors based on features from deep neural networks also falls in this category 23,24 . Indeed, the best algorithms for challenging benchmarks in pose estimation of humans from images use deep features 19,2526272829 . This suggests that deep learning architectures should also greatly improve the accuracy of pose estimation for lab applications. However, the labeled datasets for these benchmarks are large (for example, 25,000 in the MPII Human Pose dataset 30 ), which may render deep learning approaches infeasible as efficient tools at the scale of interest to neuroscience labs. Nevertheless, as a result of transfer learning 31323334 , we will show that this need not be the case.', 'Here we demonstrate that by capitalizing on state-of-the-art methods for detecting human limb configurations, we can achieve excellent performance on pose estimation problems in the laboratory setting with minimal training data. Specifically, we investigated the feature detector architecture from DeeperCut 26,27 , one of the best pose estimation algorithms, and demonstrate that a small number of training images (200) can be sufficient to train this network to within human-level labeling accuracy. This is possible as a result of transfer learning: the feature detectors are based on extremely deep neural networks, which were pretrained on ImageNet, a massive dataset for object recognition 24 . We also show that end-to-end training the network increases performance. Thus, by labeling only a few hundred frames, one can train tailored, robust feature detectors that are capable of localizing a variety of experimentally relevant body parts. We illustrate the power of this approach by tracking the snout, ears and tail base of a mouse during an odor-guided navigation task, multiple body parts of a fruit fly behaving in a 3D chamber, and joints of individual mouse digits during a reaching task.']\"}, {'heading': 'Results', 'content': \"['DeeperCut achieves outstanding performance on multi-human pose detection benchmarks 27 . However, to achieve this performance, its neural network architecture has been trained on thousands of labeled images. Here we focus on a subset of DeeperCut: its feature detectors, which are variations of deep residual neural']\"}, {'heading': 'DeepLabCut: markerless pose estimation of user-defined body parts with deep learning', 'content': \"['Alexander Mathis Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, but markers are intrusive, and the number and location of the markers must be determined a priori. Here we present an efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors. Remarkably, even when only a small number of frames are labeled (200), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy. networks (ResNet) 24 with readout layers that predict the location of a body part (Fig. 1). To distinguish the feature detectors from the full DeeperCut, we refer to this autonomous portion as DeepLabCut. In this paper, we evaluate the performance of DeepLabCut for posture tracking in various laboratory behaviors, investigate the amount of required training data for good generalization, and provide an open source toolbox that is broadly accessible to the community (https:github.comAlexEMGDeepLabCut).', 'DeepLabCut is a deep convolutional network combining two key ingredients from algorithms for object recognition and semantic segmentation: pretrained ResNets and deconvolutional layers 27 . The network consists of a variant of ResNets, whose weights were trained on a popular, large-scale object recognition benchmark called ImageNet, on which it achieves excellent performance 24 . Instead of the classification layer at the output of the ResNet, deconvolutional layers are used to up-sample the visual information and produce spatial probability densities. For each body part, its probability density represents the evidence that a body part is in a particular location. To fine-tune the network for a particular task, its weights are trained on labeled data, which consist of frames and the accompanying annotated body part locations (or other objects of interest in the frame). During training, the weights are adjusted in an iterative fashion such that for a given frame the network assigns high probabilities to labeled body part locations and low probabilities elsewhere (Fig. 1 and Methods). Thereby, the network is rewired and learns feature detectors for the labeled body parts. As a result of the initialization with the ResNet pretrained on ImageNet, this rewiring is robust and data-efficient.', 'Benchmarking DeepLabCut. Analyzing videos taken in a dynamically changing environment can be challenging. Therefore, to test the utility of our toolbox, we first focused on an odor-guided navigation task for mice. Briefly, mice run freely on an endless paper spool that includes an adapted ink-jet printer to deliver odor trails in real time as a mouse runs and tracks trails (further details and results will be published elsewhere). The video captured during the behavior poses several key challenges: inhomogeneous illumination; transparent side walls that appear dark; shadows around the mouse from overhead lighting; distortions due to a wide-angle lens; the frequent crossing of the mouse over the odor trail; and the common occurrence of rewards directly in front of its snout, which influences its appearance. Yet accurately tracking the snout as a mouse samples the odorscape is crucial for studying odor-guided navigation. Various measures could be taken to remedy these challenges, such as performing a camera calibration to reduce distortions. However, we were interested in testing whether DeepLabCut could cope with all the challenges in the raw data without any preprocessing.', 'First, we extracted 1,080 distinct frames from multiple videos (across 2 cameras and 7 different mice; see Methods) and manually labeled the snout, left ear, right ear and tail base in all frames (Fig. 2a and Supplementary Fig. 1). To facilitate comparisons to ground truth and to quantify the robustness of predictors, we estimated variability (root mean square error; RMSE) of one human labeler by comparing two distinct label sets of the same data. We found the average variability for all body parts to be very small: 2.69  0.1 pixels (mean  s.e.m.; n  4,320 body part image pairs; Supplementary Fig. 1 and Methods), which is less than the 5-pixel width of the mouses snout in low-resolution camera frames (Fig. 2a). The RMSE across two trials of annotating the same images is referred to as human variability (note that the variability differs slightly across body parts). To quantify the feature detectors performance, we randomly split the data into a training and test set (80 and 20, respectively) and evaluated the performance of DeepLabCut on test images across all body parts (Fig. 2b,c) and in a subset of body parts (snout and tail base) (Fig. 2d). Unless otherwise noted, we always trained (and tested) with the labels from the first set of human labels. The test RMSE for different trainingtest set splits achieved average human variability (Fig. 2d). Thus, we found that when trained with 80 of the data the algorithm achieved human-level accuracy on the test set for detection of the snout and the tail base (Fig. 2d,e).', 'Next, we systematically varied the size of the training set and trained 30 distinct networks (three splits for 50 and 80 training set size; six splits for 1, 5, 10 and 20 training set fraction). As expected, the test error increases for decreasing number of training images (Fig. 2f). Yet remarkably, the test RMSE attenuates only slowly from 80 training set fraction to 10, where one still achieves an average pixel error of less than 5 pixels. Such average errors are on the order of the size of the snout in the low-resolution camera (around 5 pixels) and much smaller than the size of the snout in the high-resolution camera (around 30 pixels). Thus, we found that even 100 frames were enough to achieve excellent generalization.', 'Since the RMSE is computed as an average across images, we next checked whether there were any systematic differences across images by comparing the human variability across the two splits vs. the model variability (trained with the first set of human labels; Fig. 2g: data for one split with a 50 training set size). We found that both the human and the algorithm produced only a few outliers, and no systematic error was detected (see Fig. 2g for examples).', 'We also tested whether data augmentation beyond rescaling (see Methods) could provide any gains in performance. On larger training sets DeepLabCut already reaches human-level accuracy, so we focused on the six splits that used only 10 of the data for training. Specifically, we augmented the data to include either several rotations or several translations plus rotations per training image (see Methods). We found minimal differences in test performance (Supplementary Fig. 2a), highlighting the data-efficiency of DeepLabCut. This also suggests that simple data augmentation cannot replace images that capture behavioral variability-i.e., adding new labeled images that have more postural variability seems to be better than augmenting a smaller subset of the data.', 'Thus far, we used a part detector based on the 50-layer deep ResNet-50 24,27 . We also trained deeper networks with 101 layers and 2b).', 'Overall, given the robustness and the low error rate of DeepLabCut even with small training sets, we found this to be a useful tool for studies such as odor-guided navigation. For example, Fig. 3 recapitulates a salient signature of the tracking behavior, namely that rodents swing their snout across the trail 35 . Knowing the location of the ears as well as the tail base is also important to computationally assess the orientation of the mouse (Fig. 3 and Supplementary Video 1). Furthermore, having an automated pose estimation algorithm as presented will be crucial for other videorich experiments.', 'Generalization and transfer learning. We have demonstrated that DeepLabCut can accurately detect body parts across different mice, but how does it generalize to novel scenarios? First, we found that DeepLabCut generalizes to novel mice during trail tracking (Fig. 4a). Second, we tested whether the trained network could identify multiple body parts across multiple mice in the same frame (transfer learning). Notably, although the network has only been trained with images containing a single mouse, it could detect all the body parts of each mouse in images with multiple interacting mice. Although not error-free, we found that the model performed remarkably well in a social task (three mice interacting in an unevenly illuminated open field; Fig. 4b). The performance of the body part detectors could be improved by training the network with training images that include multiple mice with occlusions andor by training image-conditioned pairwise terms between body parts to harness the power of multi-human pose estimation models 27 (see Discussion). Nonetheless, this example of multi-mouse tracking illustrates that even the feature detectors trained on a single mouse can readily transfer to extensions, as would be useful in studies of social behaviors 6,36,37 .', 'The power of end-to-end training. As a result of the architecture of DeeperCut, the deconvolution layers are specific to each body part, but the deep network (ResNet) is shared (Fig. 1). We hypothesized that this architecture can facilitate the localization of one body part based on other labeled body parts. To test this hypothesis, we examined the performance of networks trained with only the snout and tail-base data while using the same three splits of 50 training data as in Fig. 2b. We found that the network that was trained with all body part labels simultaneously outperforms the specialized networks nearly twofold (Fig. 5). This result also demonstrates that training the weights throughout the whole network in an endto-end fashion rather than just the readout weights substantially improves the performance. This further highlights the advantage of deep learning based models over approaches with fixed feature representations, which cannot be refined during training.', 'Drosophila in a 3D behavioral chamber. To further demonstrate the flexibility of the DeepLabCut toolbox, we tracked the bodies of freely behaving fruit flies (Drosophila) exploring a small cubical environment in which one surface contained an agar-based substrate for egg laying. Freely behaving flies readily exhibit many orientations and also frequent the walls and ceiling. When viewed from a fixed perspective, these changes in orientation dramatically alter the appearance of flies as the spatial relationship of body features change or as different body parts come into or out of view. Moreover, reliably tracking features across an entire egg-laying behavioral session could potentially be challenging to DeepLabCut owing to significant changes in the background (the accumulation of new eggs or changes in the agar substrate appearance due to evaporation).', 'To build toward an understanding of the behavioral patterns that surround egg-laying in an efficient way, we chose 12 distinct points on the body of the fly and labeled 589 frames of diverse orientation and posture from six different animals, labeling only those features that were visible within a given frame (see Methods).', 'We trained DeepLabCut with 95 of the data and found a test error of 4.17  0.32 pixels (mean  s.e.m.; corresponding to an average training error of 1.39  0.01 pixels, n  3 splits, mean  s.e.m.). For reference, the average eye diameter (top to bottom) was 36 pixels and the average femur diameter was 8.5 pixels (although owing to the 3D body movements and chamber depth, sizes change depending on the flys location). Figure 6a depicts some example test frames with human-and network-applied labels. Generalization to flies not used in the training set was excellent, and the feature detectors were robust to changes in orientation (Fig. 6b) and background (Fig. 6c and Supplementary Video 2). Although fly bodies are relatively rigid, which simplifies tracking, there are exceptions. For instance, the proboscis changes its visual appearance substantially during feeding behaviors. Yet the feature detectors can resolve fast motor movements such as the extension and retraction of the proboscis (Fig. 6d). Thus, DeepLabCut allows accurate extraction of low-dimensional pose information from videos of freely behaving Drosophila.', 'Digit tracking during reaching. To further illustrate the versatility and capabilities of DeepLabCut, we tracked segments of individual digits of a mouse hand (Figs 1 and7a). We recently established a head-fixed, skilled reaching task in mice 38 , wherein mice grab a joystick with two degrees of freedom and pull it from a start location to a target location. While the joystick allows spatio-temporally accurate measurement of the joystick (hand) position during the pull, it neither constrains the hand position on the joystick nor provides position information during the reaches or between pulls (when the mice might or might not hold the joystick). Placing markers is difficult as the mouse hand is a small and highly complex structure with multiple bones, joints and muscles. Moreover, it is intrusive to mice and can disrupt performance. Therefore, in principle, markerless tracking is a promising approach for analyzing reaching dynamics. However, tracking is challenging because of the complexity of possible hand articulations, as well as the presence of the other hand in the background, making this task well suited to highlighting the generality of our DeepLabCut toolbox.', 'We labeled 13 body parts: 3 points per visible digit and the wrist (see Methods). Notably, we found that by using just 141 training frames we achieved an average test error of 5.21  0.28 pixels (mean  s.e.m.; corresponding to average training error 1.16  0.03 pixels, n  3 splits, mean  s.e.m.). For reference, the width of a single digit was 15 pixels. Figure 7a depicts some example test frames.', 'We believe that this application of hand pose estimation highlights the excellent generalization performance of DeepLabCut despite training with only a few images.', 'So far we have shown that the body part estimates derived from DeeperCut are highly accurate. But, in general, especially when sieving through massive datasets, the end user would like to have each point estimate accompanied by a confidence measure of the label location. The location predictions in DeeperCut are obtained by extracting the most likely region, based on a scalar field that represents the probability that a particular body part is in a particular region. In DeeperCut these probability maps are called score-maps, and predictions are generated by finding the point with the highest probability value (see Methods). The amplitude of the maximum can be used as a confidence readout to examine the strength of evidence for individual localizations of the individual parts to be detected. For instance, the peak probability of the digit tip is low when the mouse holds the joystick (in which case the finger tips are occluded). Similarly, when the features cannot be disambiguated, the likelihood becomes small (Fig. 7a). This confidence readout also works in other contexts: for instance, in the Drosophila example frames we only depicted the predicted body parts when the probability was larger than 10 (Fig. 6b-d). Using this threshold, the point estimate for the left leg was automatically excluded in Fig. 6c,d. Indeed, all occluded body parts are also omitted in Figs 6b and7f.', 'Lastly, once a network is trained on the hand posture frames, the body part locations can be extracted from videos and used in many ways. Here we illustrate a few examples: digit positions during a reach across time (Fig. 7b; note that this trajectory comes from frame-by-frame prediction without any temporal filtering), comparison of movement patterns across body parts (Fig. 7c,d), dimensionality reduction to reveal the richness of mouse hand postures during reaching (Fig. 7e) and creating skeletons based on the semantic meaning of the labels (Fig. 7f and Supplementary Video 3).']\"}, {'heading': 'Discussion', 'content': \"['Detecting postures from monocular images is a challenging problem. Traditionally, postures are modeled as a graph of parts, where each node encodes the local visual properties of the part in question, and then these parts are connected by spring-like links. This graph We trained specialized networks with only the snout or tail-base labels, respectively. We compare the RMSE against the full model that was trained on all body parts, but is also only evaluated on the snout or tail base, respectively (e.g., fullsnout means the model trained with all body parts (full) and RMSE evaluated for the snout labels). Training (blue) and test (red) performance for the full model and specialized models trained with the same n  3 50 training set splits of the data (crosses) and average RMSE (dots). Although all networks have exactly the same information about the location of the snout or tail base during training, the network that also received information about the other body parts outperforms the specialized networks.', 'is then fit to images by minimizing some appearance cost function 18 . This minimization is hard to solve, but designing the model topology together with the visual appearance model is even more challenging 18,19 ; this can be illustrated by considering the diversity of fruit fly orientations (Fig. 6) and hand postures we examined (Fig. 7).', 'In contrast, casting this problem as a minimization with deep residual neural networks allows each joint predictor to have more than just local access to the image 19,25262728 . As a result of the extreme depth of ResNets, architectures such as DeeperCut have large receptive fields, which can learn to extract postures in a robust way 27 .', 'Here we demonstrate that cutting-edge deep learning models can be efficiently used in the laboratory. Specifically, we leveraged the fact that adapting pretrained models to new tasks can dramatically reduce the amount of training data required, a phenomena known as transfer learning 27,31323334 . We first estimated the accuracy of a human labeler, who could readily identify the body parts of interest for odorguided navigation, and then demonstrated that a deep architecture can achieve similar performance on detection of body parts such as the snout or the tail base after training on only a few hundred images. Moreover, this solution requires no computational body model, stick figure, time information or sophisticated inference algorithm. Thus, it can be quickly applied to completely different behaviors that pose qualitatively distinct computer vision challenges, such as skilled reaching in mice or egg-laying in Drosophila.', 'We believe that DeepLabCut will supplement the rich literature of computational methods for video analysis 6,8,16,20212239404142 , where powerful feature detectors of user-defined body parts need to be learned for a specific situation or where regressors based on standard image features and thresholding heuristics 6,37,41 fail to provide satisfying solutions. This is particularly the case in dynamic visual environments-for example, those with varying background and reflective walls (Figs 2 and6)-or when tracking highly articulated objects such as the hand (Fig. 7).', 'Dataset labeling and fine-tuning. Deep learning algorithms are extremely powerful and can learn to associate arbitrary categories to images 33,43 . This is consistent with our own observation that the training set should be free of errors (Fig. 2g) and approximate the diversity of visual appearances. Thus, to train DeepLabCut for specific applications, we recommend labeling maximally diverse images (i.e., different poses, different individuals, luminance conditions, etc.) in a consistent way and curating the labeled data well. The training data should reflect the breadth of the experimental data to be analyzed. Even for an extremely small training set, the typical errors can be small, but large errors for test images that are quite distinct from the training set can start to dominate the average error. One limitation for generalizing to novel situations comes from stochasticity in training set selection. Given that we only select a small number of training samples (a few hundred frames), it is plausible that images representing behaviors that are especially sparse or noisy (e.g., due to motion blur) could be suboptimally sampled or entirely excluded from the training data, resulting in difficulties at test time.', 'Therefore, a user can expand the initial training dataset in an iterative fashion using the score-maps. Specifically, errors can be addressed via post hoc fine-tuning of the network weights, taking advantage of the fact that the network outputs confidence estimates for its own generated labels (Fig. 7a and Methods). By using these confidence estimates to select sequences of frames containing a rare behavior (by sampling around points of high probability), or to find frames where reliably captured behaviors are largely corrupted with noise (by sampling points of low probability), a user can then selectively label frames based on these confidence criteria to generate a minimal yet additional training set for fine-tuning the network. Additionally, heuristics such as the continuity of body part trajectories can be used to select frames with errors. This selectively improves model performance on edge cases, thereby extending the architectures capacity for generalization in an efficient way. Such an active learning framework can be used to achieve a predefined level of confidence for all images with minimal labeling cost. Then, owing to the large capacity of the neural network that underlies the feature detectors, one can continue training the network with these additional examples.', 'We note, however, that not every low value in a probability score-map necessarily reflects erroneous detection. As we showed, low probabilities can also be indicative of occlusions, as in the case of the digit tips when the mouse is holding the joystick (Fig. 7). Here, multiple camera angles can be used to fully capture a behavior of interest, or heuristics (such as a body model) can be used to approximate occluded body parts using temporal and spatial information.', 'Speed and accuracy of DeepLabCut. Another important feature of DeepLabCut is that it can accurately transform large videos into low-dimensional time sequence data with semantic meaning, as the experimenter preselects the parts that will presumably provide the most information about the behavior being studied. In contrast to high-dimensional videos, such low-dimensional time sequence data are also highly amenable to behavioral clustering and analysis because of their computational tractability 67844 . On modern hardware, pose extraction is also fast. For instance, one can process the 682  540 pixel frames of the Drosophila behavior at around 30 Hz on an NVIDIA 1080Ti GPU. Processing speeds scale with the frame size; for example, lower resolution videos with 204  162 pixel frames are analyzed at around 85 Hz. Such fast pose extraction can make this tool potentially amenable for real-time feedback 38,45 based on video-based posture estimates. This processing speed can be further improved by cropping input frames in an adaptive way around the animal andor adapting the network architecture to speed up processing times.', 'Extensions. As presented, DeepLabCut extracts the posture data frame by frame, but one can add temporal filtering to improve performance (as for other approaches) 6,46,47 . Here we omitted such methods because of the high precision of the model without these additional steps, as well as to highlight the accurate prediction based on single frames solely driven by within-frame visual information in a variety of contexts. While temporal information could indeed be beneficial in certain contexts, challenges remain to using end-to-end-trained deep architectures for video data to extract postures. Because of the curse of dimensionality, deep architectures on videos must rely on input images with lower spatial resolution, and thus the best-performing action recognition algorithms still rely on frame-by-frame analysis with deep networks pretrained on ImageNet as a result of hardware limitations 28,29,48 . As this is an active area of research, we believe this situation is likely to change with improvements in hardware (and in deep learning algorithms), and this should have a strong influence on pose estimation in the future. Therefore currently, in situations where occlusions are very common, such as in social behaviors, pairwise interactions could also be added to improve performance 6,13141516171827,29 . Here we have focused on the deep feature detectors alone to demonstrate remarkable transfer learning for laboratory tasks without the need for such extensions.']\"}, {'heading': 'Conclusions', 'content': \"['Together with this report, we provide an open source software package called DeepLabCut. The toolbox uses the feature detectors from DeeperCut and provides routines to (i) extract distinct frames from videos for labeling, (ii) generate training data based on labels, (iii) train networks to the desired feature sets, and (iv) extract these feature locations from unlabeled data (Fig. 1). The typical use case would be for an experimenter to extract distinct frames from videos and label the body parts of interest to create tailored part detectors. Then, after only a few hours of labeling and training the network, DeepLabCut can be applied to novel videos. While we demonstrate the utility of this toolbox on mice and Drosophila, there is no inherent limitation of this framework, and our toolbox can be applied to other model, or non-model, organisms in a diverse range of behaviors.']\"}, {'heading': 'Methods', 'content': \"['DeepLabCut toolbox. This publication is accompanied by open source Python code for selecting training frames, checking human annotator labels, generating training data in the required format, and evaluating the performance on test frames. The toolbox also contains code to extract postures from novel videos with trained feature detectors. Thus, this toolbox allows one to train a tailored network based on labeled images and to perform automatic labeling for novel data. See https:github.comAlexEMGDeepLabCut for details.', 'While the presented behaviors were recorded in grayscale under infrared or normal lighting conditions, DeepLabCut can also be used for color videos. There is no inherent limitation to the cameras that can be used to collect videos that can subsequently be analyzed with our toolbox. Please see http:www.mousemotorlab. orgdeeplabcut for more example behaviors (including examples of color videos).', 'Mouse odor trail-tracking. The trail-tracking behavior is part of an investigation into odor-guided navigation wherein one or more wild-type (C57BL6J) male mice run on a paper spool following odor trails. These experiments were carried out in the laboratory of Venkatesh Murthy at Harvard University and will be published elsewhere. For trail-tracking, we extracted 1,080 random, distinct frames from multiple experimental sessions observing 7 different mice. Data were recorded at 30 Hz by two different cameras: the 640  480 pixels images were acquired with a Point Grey Firefly FMVU-03MTM-CS and the 1,700  1,200 pixel images with a Point Grey Grasshopper 3 4.1MP Mono USB3 Vision, CMOSIS CMV4000-3E12. The latter images are prohibitively large to process without downsampling, and therefore we cropped around mice to generate images that were approximately 800  800 pixels. One human annotator was instructed to localize the snout, the tip of the left and right ear and the base of the tail in the example images on two different occasions (using Fiji 49 ), which generated two distinct label sets ( 1 month apart to reduce memory bias; see Fig. 1).', 'Mouse reach and pull joystick task. Experimental procedures for the training of the joystick behavior and the construction of the behavioral set-up can be found in Mathis et al. 38 . In brief, head-fixed mice were trained to reach, grab and pull a joystick for a liquid reward. To generate a traintest set of images, we labeled 159 frames at 13 locations: 3 points per digit-the digit tip, the joint in the middle and the base of the digit (which roughly correspond to the proximal interphalangeal joint and the metacarpophalangeal joint, respectively)-as well as the base of the hand (wrist). The data were collected across 5 different mice (C57BL6J, male and female) and were recorded at 2,048  1,088 resolution with a frame rate of 100-320 Hz. For tracking the digits, we used the supplied toolbox code to crop the data to extract only regions of interest containing the movement of the forelimb to limit the size of the input image to the network.', 'All surgical and experimental procedures for mice were in accordance with the National Institutes of Health Guide for the Care and Use of Laboratory Animals and approved by the Harvard Institutional Animal Care and Use Committee.', 'Drosophila egg-laying behavior. Experiments were carried out in the laboratory of Richard Axel at Columbia University and will be published elsewhere. In brief, egg-laying behavior in female Drosophila (Canton-S strain) was observed in customdesigned 3D-printed chambers (Protolabs). Individual chambers were 4.1 mm deep and tapered from top to bottom, with the top dimensions 7.3 mm  5.8 mm and the bottom dimensions 6.7 mm  4.3 mm. One side of the chamber opened to a reservoir within which 1 agar was poured and allowed to set. Small acrylic windows were slid into place along grooves at the top and bottom to enclose the fly within the chamber and to allow viewing. The chamber was illuminated by a 2-inch off-axis ring light (Metaphase) and video recording was performed from above the chamber using an infrared-sensitive CMOS camera (Basler) with a 0.5 telecentric lens (Edmund Optics) at 20 Hz (682  540 pixels). We identified 12 distinct points of interest to quantify the behavior of interest on the body of the fly. One human annotator manually extracted 589 distinct and informative frames from six different animals, labeling only those features that were visible within a given frame. The 12 points comprise 4 points on the head (the dorsal tip of each compound eye, the ocellus and the tip of the proboscis), the posterior tip of the scutellum on the thorax, the joint between the femur and tibia on each metathoracic leg, the abdominal stripes on the four most posterior abdominal segments (A3-A6) and the ovipositor.', 'Labeled dataset set selection. No statistical methods were used to predetermine sample sizes for labeled frames, but our sample sizes are similar to those reported in previous publications. The labelers were blinded to whether the frames would be assigned to training or test datasets (as the frames were randomized across splits). For each replicate (i.e., split of the dataset), frames were randomly assigned to the test or training set. No data or experimental animals (mice or Drosophila) were excluded from the study.', 'Deep feature detector architecture. We employ strong body part detectors, which are part of state-of-the art algorithms for human pose estimation called DeeperCut 26,27,29 . Those part detectors build on state-of-the-art object recognition architectures, namely extremely deep residual networks (ResNet) 24 . Specifically, we use a variant of the ResNet with 50 layers, which achieved outstanding performance in object recognition competitions 24 . In the DeeperCut implementation, the ResNets were adapted to represent the images with higher spatial resolution, and the softmax layer used in the original architecture after the conv5 bank (as would be appropriate for object classification) was replaced by deconvolutional layers that produce a scalar field of activation values corresponding to regions in original image. This output is also connected to the conv3 bank to make use of finer features generated earlier in the ResNet architecture 27 . For each body part, there is a corresponding output layer whose activity represents probability score-maps . These score-maps represent the probability that a body part is at a particular pixel 26,27 . During training, a score-map with positive label 1 (unity probability) is generated for all locations up to ϵ pixels away from the ground truth per body part (distance variable). The ResNet architecture used to generate features is initialized with weights trained on ImageNet 24 , and the cross-entropy loss between the predicted score-map and the ground-truth score-map is minimized by stochastic gradient descent 27 . Around 500,000 training steps were enough for convergence in the presented cases, and training takes up to 24 h on a GPU (NVIDIA GTX 1080 Ti; note that typically the loss starts to slowly decay early in training; see Fig. 2b). We used a batch size of 1, which allows us to have images of different sizes, decreased the learning rate over training and performed data augmentation during training by rescaling the images (as in DeeperCut, but we used a range of 50 to 150). We also tested further data augmentation by additionally training with 7 rotated frames per training image (rotation group-angles independently and uniformly sampled (uid) from -8,8 degrees) as well as 9 rotated and 14 partial images per training images (rotation and translation group-angles uid from -10,10 degrees, as well as uid subimages amounting to relative shifts). Unless otherwise noted, we used a distance variable ϵ  17 (pixel radius) and scale factor 0.8 (which affects the ratio of the input image to output score-map). We cross-validated the choice of ϵ for a higher resolution output (scale factor  1) and found that the test performance was not improved when varying ϵ widely, but the rate of performance improvement was strongly decreased for small ϵ (Supplementary Fig. 2). We also evaluated deeper networks with 101 layers, ResNet-101, as well as ResNet-101ws (with intermediate supervision, Supplementary Fig. 2b); more technical details can be found in Insafutdinov et al. 27 .', 'Evaluation and error measures. The trained network can be used to predict body part locations. At any state of training, the network can be presented with novel frames, for which the prediction of the location of a particular body part is given by the peak of the corresponding score-map. This estimate is further refined on the basis of learned correspondences between the score-map grid and ground truth joint positions 26,27,29 . In the case of multiple mice, the local maxima of the scoremap are extracted as predictions of the body part locations (Fig. 4).', 'As discussed in the main text, a user can continue to fine-tune the network for increasing generalization to large datasets to reduce errors. One can use features of the score-maps such as the amplitude and width, or heuristics such as the continuity of body part trajectories, to identify images for which the decoder might make large errors. Images with insufficient automatic labeling performance that are identified in this way can then be manually labeled to increase the training set and iteratively improve the feature detectors.', 'To compare between datasets generated by the human scorer, as well as with or between model-generated labels, we used the Euclidean distance (root mean square error, RMSE) calculated pairwise per body part. Depending on the context, this metric is either shown for a specific body part, averaged over all body parts, or averaged over a set of images. To quantify the error across learning, we stored snapshots of the weights in TensorFlow 50 (usually every 50,000 iterations) and evaluated the RMSE for predictions generated by these frozen networks post hoc. Note that the RMSE is not the loss function minimized during training. However, the RMSE is the relevant performance metric for assessing labeling precision in pixels.', 'The RMSE between the first and second annotation is referred to as human variability. In figures we also depict the 95 confidence interval for this RMSE, whose limits are given as mean  1.96 times the s.e.m. (Figs 2c,d,f, 4 and 5 and Supplementary Fig. 2a-d). Depending on the figure, the RMSE is averaged over all or just a subset of body parts. For the Drosophila and the mouse hand data, we report the average test RMSE for all body parts with likelihood larger than 10.', 'In Fig. 7 we extracted cropped images of the hand from full frames (n  1,139) by centering it using the predicted wrist position. We then performed dimensionality reduction by t-SNE embedding of those images 51 and randomly selected certain sufficiently distant points to illustrate the corresponding hand postures.', 'Reporting Summary. Further information on experimental design is available in the Nature Research Reporting Summary linked to this article.']\"}]} Each input should yield **one resource** only. If the input mentions other resources, include them under a `mentions` field but do not extract them as separate entries.\n",
      "Return the following fields in JSON format:\n",
      "  - name\n",
      "  - description\n",
      "  - type (Model, Dataset, Tool, Paper, Benchmark, Leaderboard)\n",
      "  - category (e.g., Pose Estimation, Behavioral Quantification)\n",
      "  - target (e.g., Animal, Human, Mammals)\n",
      "  - specific target (e.g., Mice, Macaque, Bird)\n",
      "  - url\n",
      "  - mentions (optional: dictionary with fields like models, datasets, benchmarks, papers)\n",
      "\n",
      "Use `null` if information is missing.\n",
      "Resource: {'metadata': {'title': '', 'authors': ['Mackenzie Weygandt Mathis'], 'abstract': '', 'publication_date': 'April 2018'}, 'sections': [{'heading': 'Untitled Section 1', 'content': \"['A ccurate quantification of behavior is essential for understanding the brain 123 . Both within and beyond the field of neuroscience, there is a fruitful tradition of using cuttingedge technology to study movement. Often, the application of new technology has the potential to reveal unforeseen features of the phenomena being studied, as in the case of Muybridges famous photography studies in the mid-19th century or modern highspeed videography that has revealed previously unknown motor sequences, such as tap dancing in the songbird 2,4,5 . Historically, collected data were analyzed manually, which is a time-consuming, labor-intensive and error-prone process that is prohibitively inefficient at todays high rates of data acquisition. Conversely, advances in computer vision have consistently inspired methods of data analysis to reduce human labor 678 .', 'We are particularly interested in extracting the pose of animals-i.e., the geometrical configuration of multiple body parts. The gold standard for pose estimation in the field of motor control is the combination of video recordings with easily recognizable reflective markers applied to locations of interest, which greatly simplifies subsequent analysis and allows tracking of body parts with high accuracy 9101112 . However, such systems can be expensive and potentially distracting to animals 13,14 , and markers need to be placed before recording, which predefines the features that can be tracked. This mitigates one of the benefits of video data: its low level of invasiveness. One alternative to physical markers is to fit skeleton or active contour models 1314151617 . These methods can work quite well and are fast, but require sophisticated skeleton models, which are difficult to develop and to fit to data, limiting the flexibility of such methods 18,19 . Another alternative is training regressors based on various computationally derived features to track particular body parts in a supervised way 6,13,202122 . Training predictors based on features from deep neural networks also falls in this category 23,24 . Indeed, the best algorithms for challenging benchmarks in pose estimation of humans from images use deep features 19,2526272829 . This suggests that deep learning architectures should also greatly improve the accuracy of pose estimation for lab applications. However, the labeled datasets for these benchmarks are large (for example, 25,000 in the MPII Human Pose dataset 30 ), which may render deep learning approaches infeasible as efficient tools at the scale of interest to neuroscience labs. Nevertheless, as a result of transfer learning 31323334 , we will show that this need not be the case.', 'Here we demonstrate that by capitalizing on state-of-the-art methods for detecting human limb configurations, we can achieve excellent performance on pose estimation problems in the laboratory setting with minimal training data. Specifically, we investigated the feature detector architecture from DeeperCut 26,27 , one of the best pose estimation algorithms, and demonstrate that a small number of training images (200) can be sufficient to train this network to within human-level labeling accuracy. This is possible as a result of transfer learning: the feature detectors are based on extremely deep neural networks, which were pretrained on ImageNet, a massive dataset for object recognition 24 . We also show that end-to-end training the network increases performance. Thus, by labeling only a few hundred frames, one can train tailored, robust feature detectors that are capable of localizing a variety of experimentally relevant body parts. We illustrate the power of this approach by tracking the snout, ears and tail base of a mouse during an odor-guided navigation task, multiple body parts of a fruit fly behaving in a 3D chamber, and joints of individual mouse digits during a reaching task.']\"}, {'heading': 'Results', 'content': \"['DeeperCut achieves outstanding performance on multi-human pose detection benchmarks 27 . However, to achieve this performance, its neural network architecture has been trained on thousands of labeled images. Here we focus on a subset of DeeperCut: its feature detectors, which are variations of deep residual neural']\"}, {'heading': 'DeepLabCut: markerless pose estimation of user-defined body parts with deep learning', 'content': \"['Alexander Mathis Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, but markers are intrusive, and the number and location of the markers must be determined a priori. Here we present an efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors. Remarkably, even when only a small number of frames are labeled (200), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy. networks (ResNet) 24 with readout layers that predict the location of a body part (Fig. 1). To distinguish the feature detectors from the full DeeperCut, we refer to this autonomous portion as DeepLabCut. In this paper, we evaluate the performance of DeepLabCut for posture tracking in various laboratory behaviors, investigate the amount of required training data for good generalization, and provide an open source toolbox that is broadly accessible to the community (https:github.comAlexEMGDeepLabCut).', 'DeepLabCut is a deep convolutional network combining two key ingredients from algorithms for object recognition and semantic segmentation: pretrained ResNets and deconvolutional layers 27 . The network consists of a variant of ResNets, whose weights were trained on a popular, large-scale object recognition benchmark called ImageNet, on which it achieves excellent performance 24 . Instead of the classification layer at the output of the ResNet, deconvolutional layers are used to up-sample the visual information and produce spatial probability densities. For each body part, its probability density represents the evidence that a body part is in a particular location. To fine-tune the network for a particular task, its weights are trained on labeled data, which consist of frames and the accompanying annotated body part locations (or other objects of interest in the frame). During training, the weights are adjusted in an iterative fashion such that for a given frame the network assigns high probabilities to labeled body part locations and low probabilities elsewhere (Fig. 1 and Methods). Thereby, the network is rewired and learns feature detectors for the labeled body parts. As a result of the initialization with the ResNet pretrained on ImageNet, this rewiring is robust and data-efficient.', 'Benchmarking DeepLabCut. Analyzing videos taken in a dynamically changing environment can be challenging. Therefore, to test the utility of our toolbox, we first focused on an odor-guided navigation task for mice. Briefly, mice run freely on an endless paper spool that includes an adapted ink-jet printer to deliver odor trails in real time as a mouse runs and tracks trails (further details and results will be published elsewhere). The video captured during the behavior poses several key challenges: inhomogeneous illumination; transparent side walls that appear dark; shadows around the mouse from overhead lighting; distortions due to a wide-angle lens; the frequent crossing of the mouse over the odor trail; and the common occurrence of rewards directly in front of its snout, which influences its appearance. Yet accurately tracking the snout as a mouse samples the odorscape is crucial for studying odor-guided navigation. Various measures could be taken to remedy these challenges, such as performing a camera calibration to reduce distortions. However, we were interested in testing whether DeepLabCut could cope with all the challenges in the raw data without any preprocessing.', 'First, we extracted 1,080 distinct frames from multiple videos (across 2 cameras and 7 different mice; see Methods) and manually labeled the snout, left ear, right ear and tail base in all frames (Fig. 2a and Supplementary Fig. 1). To facilitate comparisons to ground truth and to quantify the robustness of predictors, we estimated variability (root mean square error; RMSE) of one human labeler by comparing two distinct label sets of the same data. We found the average variability for all body parts to be very small: 2.69  0.1 pixels (mean  s.e.m.; n  4,320 body part image pairs; Supplementary Fig. 1 and Methods), which is less than the 5-pixel width of the mouses snout in low-resolution camera frames (Fig. 2a). The RMSE across two trials of annotating the same images is referred to as human variability (note that the variability differs slightly across body parts). To quantify the feature detectors performance, we randomly split the data into a training and test set (80 and 20, respectively) and evaluated the performance of DeepLabCut on test images across all body parts (Fig. 2b,c) and in a subset of body parts (snout and tail base) (Fig. 2d). Unless otherwise noted, we always trained (and tested) with the labels from the first set of human labels. The test RMSE for different trainingtest set splits achieved average human variability (Fig. 2d). Thus, we found that when trained with 80 of the data the algorithm achieved human-level accuracy on the test set for detection of the snout and the tail base (Fig. 2d,e).', 'Next, we systematically varied the size of the training set and trained 30 distinct networks (three splits for 50 and 80 training set size; six splits for 1, 5, 10 and 20 training set fraction). As expected, the test error increases for decreasing number of training images (Fig. 2f). Yet remarkably, the test RMSE attenuates only slowly from 80 training set fraction to 10, where one still achieves an average pixel error of less than 5 pixels. Such average errors are on the order of the size of the snout in the low-resolution camera (around 5 pixels) and much smaller than the size of the snout in the high-resolution camera (around 30 pixels). Thus, we found that even 100 frames were enough to achieve excellent generalization.', 'Since the RMSE is computed as an average across images, we next checked whether there were any systematic differences across images by comparing the human variability across the two splits vs. the model variability (trained with the first set of human labels; Fig. 2g: data for one split with a 50 training set size). We found that both the human and the algorithm produced only a few outliers, and no systematic error was detected (see Fig. 2g for examples).', 'We also tested whether data augmentation beyond rescaling (see Methods) could provide any gains in performance. On larger training sets DeepLabCut already reaches human-level accuracy, so we focused on the six splits that used only 10 of the data for training. Specifically, we augmented the data to include either several rotations or several translations plus rotations per training image (see Methods). We found minimal differences in test performance (Supplementary Fig. 2a), highlighting the data-efficiency of DeepLabCut. This also suggests that simple data augmentation cannot replace images that capture behavioral variability-i.e., adding new labeled images that have more postural variability seems to be better than augmenting a smaller subset of the data.', 'Thus far, we used a part detector based on the 50-layer deep ResNet-50 24,27 . We also trained deeper networks with 101 layers and 2b).', 'Overall, given the robustness and the low error rate of DeepLabCut even with small training sets, we found this to be a useful tool for studies such as odor-guided navigation. For example, Fig. 3 recapitulates a salient signature of the tracking behavior, namely that rodents swing their snout across the trail 35 . Knowing the location of the ears as well as the tail base is also important to computationally assess the orientation of the mouse (Fig. 3 and Supplementary Video 1). Furthermore, having an automated pose estimation algorithm as presented will be crucial for other videorich experiments.', 'Generalization and transfer learning. We have demonstrated that DeepLabCut can accurately detect body parts across different mice, but how does it generalize to novel scenarios? First, we found that DeepLabCut generalizes to novel mice during trail tracking (Fig. 4a). Second, we tested whether the trained network could identify multiple body parts across multiple mice in the same frame (transfer learning). Notably, although the network has only been trained with images containing a single mouse, it could detect all the body parts of each mouse in images with multiple interacting mice. Although not error-free, we found that the model performed remarkably well in a social task (three mice interacting in an unevenly illuminated open field; Fig. 4b). The performance of the body part detectors could be improved by training the network with training images that include multiple mice with occlusions andor by training image-conditioned pairwise terms between body parts to harness the power of multi-human pose estimation models 27 (see Discussion). Nonetheless, this example of multi-mouse tracking illustrates that even the feature detectors trained on a single mouse can readily transfer to extensions, as would be useful in studies of social behaviors 6,36,37 .', 'The power of end-to-end training. As a result of the architecture of DeeperCut, the deconvolution layers are specific to each body part, but the deep network (ResNet) is shared (Fig. 1). We hypothesized that this architecture can facilitate the localization of one body part based on other labeled body parts. To test this hypothesis, we examined the performance of networks trained with only the snout and tail-base data while using the same three splits of 50 training data as in Fig. 2b. We found that the network that was trained with all body part labels simultaneously outperforms the specialized networks nearly twofold (Fig. 5). This result also demonstrates that training the weights throughout the whole network in an endto-end fashion rather than just the readout weights substantially improves the performance. This further highlights the advantage of deep learning based models over approaches with fixed feature representations, which cannot be refined during training.', 'Drosophila in a 3D behavioral chamber. To further demonstrate the flexibility of the DeepLabCut toolbox, we tracked the bodies of freely behaving fruit flies (Drosophila) exploring a small cubical environment in which one surface contained an agar-based substrate for egg laying. Freely behaving flies readily exhibit many orientations and also frequent the walls and ceiling. When viewed from a fixed perspective, these changes in orientation dramatically alter the appearance of flies as the spatial relationship of body features change or as different body parts come into or out of view. Moreover, reliably tracking features across an entire egg-laying behavioral session could potentially be challenging to DeepLabCut owing to significant changes in the background (the accumulation of new eggs or changes in the agar substrate appearance due to evaporation).', 'To build toward an understanding of the behavioral patterns that surround egg-laying in an efficient way, we chose 12 distinct points on the body of the fly and labeled 589 frames of diverse orientation and posture from six different animals, labeling only those features that were visible within a given frame (see Methods).', 'We trained DeepLabCut with 95 of the data and found a test error of 4.17  0.32 pixels (mean  s.e.m.; corresponding to an average training error of 1.39  0.01 pixels, n  3 splits, mean  s.e.m.). For reference, the average eye diameter (top to bottom) was 36 pixels and the average femur diameter was 8.5 pixels (although owing to the 3D body movements and chamber depth, sizes change depending on the flys location). Figure 6a depicts some example test frames with human-and network-applied labels. Generalization to flies not used in the training set was excellent, and the feature detectors were robust to changes in orientation (Fig. 6b) and background (Fig. 6c and Supplementary Video 2). Although fly bodies are relatively rigid, which simplifies tracking, there are exceptions. For instance, the proboscis changes its visual appearance substantially during feeding behaviors. Yet the feature detectors can resolve fast motor movements such as the extension and retraction of the proboscis (Fig. 6d). Thus, DeepLabCut allows accurate extraction of low-dimensional pose information from videos of freely behaving Drosophila.', 'Digit tracking during reaching. To further illustrate the versatility and capabilities of DeepLabCut, we tracked segments of individual digits of a mouse hand (Figs 1 and7a). We recently established a head-fixed, skilled reaching task in mice 38 , wherein mice grab a joystick with two degrees of freedom and pull it from a start location to a target location. While the joystick allows spatio-temporally accurate measurement of the joystick (hand) position during the pull, it neither constrains the hand position on the joystick nor provides position information during the reaches or between pulls (when the mice might or might not hold the joystick). Placing markers is difficult as the mouse hand is a small and highly complex structure with multiple bones, joints and muscles. Moreover, it is intrusive to mice and can disrupt performance. Therefore, in principle, markerless tracking is a promising approach for analyzing reaching dynamics. However, tracking is challenging because of the complexity of possible hand articulations, as well as the presence of the other hand in the background, making this task well suited to highlighting the generality of our DeepLabCut toolbox.', 'We labeled 13 body parts: 3 points per visible digit and the wrist (see Methods). Notably, we found that by using just 141 training frames we achieved an average test error of 5.21  0.28 pixels (mean  s.e.m.; corresponding to average training error 1.16  0.03 pixels, n  3 splits, mean  s.e.m.). For reference, the width of a single digit was 15 pixels. Figure 7a depicts some example test frames.', 'We believe that this application of hand pose estimation highlights the excellent generalization performance of DeepLabCut despite training with only a few images.', 'So far we have shown that the body part estimates derived from DeeperCut are highly accurate. But, in general, especially when sieving through massive datasets, the end user would like to have each point estimate accompanied by a confidence measure of the label location. The location predictions in DeeperCut are obtained by extracting the most likely region, based on a scalar field that represents the probability that a particular body part is in a particular region. In DeeperCut these probability maps are called score-maps, and predictions are generated by finding the point with the highest probability value (see Methods). The amplitude of the maximum can be used as a confidence readout to examine the strength of evidence for individual localizations of the individual parts to be detected. For instance, the peak probability of the digit tip is low when the mouse holds the joystick (in which case the finger tips are occluded). Similarly, when the features cannot be disambiguated, the likelihood becomes small (Fig. 7a). This confidence readout also works in other contexts: for instance, in the Drosophila example frames we only depicted the predicted body parts when the probability was larger than 10 (Fig. 6b-d). Using this threshold, the point estimate for the left leg was automatically excluded in Fig. 6c,d. Indeed, all occluded body parts are also omitted in Figs 6b and7f.', 'Lastly, once a network is trained on the hand posture frames, the body part locations can be extracted from videos and used in many ways. Here we illustrate a few examples: digit positions during a reach across time (Fig. 7b; note that this trajectory comes from frame-by-frame prediction without any temporal filtering), comparison of movement patterns across body parts (Fig. 7c,d), dimensionality reduction to reveal the richness of mouse hand postures during reaching (Fig. 7e) and creating skeletons based on the semantic meaning of the labels (Fig. 7f and Supplementary Video 3).']\"}, {'heading': 'Discussion', 'content': \"['Detecting postures from monocular images is a challenging problem. Traditionally, postures are modeled as a graph of parts, where each node encodes the local visual properties of the part in question, and then these parts are connected by spring-like links. This graph We trained specialized networks with only the snout or tail-base labels, respectively. We compare the RMSE against the full model that was trained on all body parts, but is also only evaluated on the snout or tail base, respectively (e.g., fullsnout means the model trained with all body parts (full) and RMSE evaluated for the snout labels). Training (blue) and test (red) performance for the full model and specialized models trained with the same n  3 50 training set splits of the data (crosses) and average RMSE (dots). Although all networks have exactly the same information about the location of the snout or tail base during training, the network that also received information about the other body parts outperforms the specialized networks.', 'is then fit to images by minimizing some appearance cost function 18 . This minimization is hard to solve, but designing the model topology together with the visual appearance model is even more challenging 18,19 ; this can be illustrated by considering the diversity of fruit fly orientations (Fig. 6) and hand postures we examined (Fig. 7).', 'In contrast, casting this problem as a minimization with deep residual neural networks allows each joint predictor to have more than just local access to the image 19,25262728 . As a result of the extreme depth of ResNets, architectures such as DeeperCut have large receptive fields, which can learn to extract postures in a robust way 27 .', 'Here we demonstrate that cutting-edge deep learning models can be efficiently used in the laboratory. Specifically, we leveraged the fact that adapting pretrained models to new tasks can dramatically reduce the amount of training data required, a phenomena known as transfer learning 27,31323334 . We first estimated the accuracy of a human labeler, who could readily identify the body parts of interest for odorguided navigation, and then demonstrated that a deep architecture can achieve similar performance on detection of body parts such as the snout or the tail base after training on only a few hundred images. Moreover, this solution requires no computational body model, stick figure, time information or sophisticated inference algorithm. Thus, it can be quickly applied to completely different behaviors that pose qualitatively distinct computer vision challenges, such as skilled reaching in mice or egg-laying in Drosophila.', 'We believe that DeepLabCut will supplement the rich literature of computational methods for video analysis 6,8,16,20212239404142 , where powerful feature detectors of user-defined body parts need to be learned for a specific situation or where regressors based on standard image features and thresholding heuristics 6,37,41 fail to provide satisfying solutions. This is particularly the case in dynamic visual environments-for example, those with varying background and reflective walls (Figs 2 and6)-or when tracking highly articulated objects such as the hand (Fig. 7).', 'Dataset labeling and fine-tuning. Deep learning algorithms are extremely powerful and can learn to associate arbitrary categories to images 33,43 . This is consistent with our own observation that the training set should be free of errors (Fig. 2g) and approximate the diversity of visual appearances. Thus, to train DeepLabCut for specific applications, we recommend labeling maximally diverse images (i.e., different poses, different individuals, luminance conditions, etc.) in a consistent way and curating the labeled data well. The training data should reflect the breadth of the experimental data to be analyzed. Even for an extremely small training set, the typical errors can be small, but large errors for test images that are quite distinct from the training set can start to dominate the average error. One limitation for generalizing to novel situations comes from stochasticity in training set selection. Given that we only select a small number of training samples (a few hundred frames), it is plausible that images representing behaviors that are especially sparse or noisy (e.g., due to motion blur) could be suboptimally sampled or entirely excluded from the training data, resulting in difficulties at test time.', 'Therefore, a user can expand the initial training dataset in an iterative fashion using the score-maps. Specifically, errors can be addressed via post hoc fine-tuning of the network weights, taking advantage of the fact that the network outputs confidence estimates for its own generated labels (Fig. 7a and Methods). By using these confidence estimates to select sequences of frames containing a rare behavior (by sampling around points of high probability), or to find frames where reliably captured behaviors are largely corrupted with noise (by sampling points of low probability), a user can then selectively label frames based on these confidence criteria to generate a minimal yet additional training set for fine-tuning the network. Additionally, heuristics such as the continuity of body part trajectories can be used to select frames with errors. This selectively improves model performance on edge cases, thereby extending the architectures capacity for generalization in an efficient way. Such an active learning framework can be used to achieve a predefined level of confidence for all images with minimal labeling cost. Then, owing to the large capacity of the neural network that underlies the feature detectors, one can continue training the network with these additional examples.', 'We note, however, that not every low value in a probability score-map necessarily reflects erroneous detection. As we showed, low probabilities can also be indicative of occlusions, as in the case of the digit tips when the mouse is holding the joystick (Fig. 7). Here, multiple camera angles can be used to fully capture a behavior of interest, or heuristics (such as a body model) can be used to approximate occluded body parts using temporal and spatial information.', 'Speed and accuracy of DeepLabCut. Another important feature of DeepLabCut is that it can accurately transform large videos into low-dimensional time sequence data with semantic meaning, as the experimenter preselects the parts that will presumably provide the most information about the behavior being studied. In contrast to high-dimensional videos, such low-dimensional time sequence data are also highly amenable to behavioral clustering and analysis because of their computational tractability 67844 . On modern hardware, pose extraction is also fast. For instance, one can process the 682  540 pixel frames of the Drosophila behavior at around 30 Hz on an NVIDIA 1080Ti GPU. Processing speeds scale with the frame size; for example, lower resolution videos with 204  162 pixel frames are analyzed at around 85 Hz. Such fast pose extraction can make this tool potentially amenable for real-time feedback 38,45 based on video-based posture estimates. This processing speed can be further improved by cropping input frames in an adaptive way around the animal andor adapting the network architecture to speed up processing times.', 'Extensions. As presented, DeepLabCut extracts the posture data frame by frame, but one can add temporal filtering to improve performance (as for other approaches) 6,46,47 . Here we omitted such methods because of the high precision of the model without these additional steps, as well as to highlight the accurate prediction based on single frames solely driven by within-frame visual information in a variety of contexts. While temporal information could indeed be beneficial in certain contexts, challenges remain to using end-to-end-trained deep architectures for video data to extract postures. Because of the curse of dimensionality, deep architectures on videos must rely on input images with lower spatial resolution, and thus the best-performing action recognition algorithms still rely on frame-by-frame analysis with deep networks pretrained on ImageNet as a result of hardware limitations 28,29,48 . As this is an active area of research, we believe this situation is likely to change with improvements in hardware (and in deep learning algorithms), and this should have a strong influence on pose estimation in the future. Therefore currently, in situations where occlusions are very common, such as in social behaviors, pairwise interactions could also be added to improve performance 6,13141516171827,29 . Here we have focused on the deep feature detectors alone to demonstrate remarkable transfer learning for laboratory tasks without the need for such extensions.']\"}, {'heading': 'Conclusions', 'content': \"['Together with this report, we provide an open source software package called DeepLabCut. The toolbox uses the feature detectors from DeeperCut and provides routines to (i) extract distinct frames from videos for labeling, (ii) generate training data based on labels, (iii) train networks to the desired feature sets, and (iv) extract these feature locations from unlabeled data (Fig. 1). The typical use case would be for an experimenter to extract distinct frames from videos and label the body parts of interest to create tailored part detectors. Then, after only a few hours of labeling and training the network, DeepLabCut can be applied to novel videos. While we demonstrate the utility of this toolbox on mice and Drosophila, there is no inherent limitation of this framework, and our toolbox can be applied to other model, or non-model, organisms in a diverse range of behaviors.']\"}, {'heading': 'Methods', 'content': \"['DeepLabCut toolbox. This publication is accompanied by open source Python code for selecting training frames, checking human annotator labels, generating training data in the required format, and evaluating the performance on test frames. The toolbox also contains code to extract postures from novel videos with trained feature detectors. Thus, this toolbox allows one to train a tailored network based on labeled images and to perform automatic labeling for novel data. See https:github.comAlexEMGDeepLabCut for details.', 'While the presented behaviors were recorded in grayscale under infrared or normal lighting conditions, DeepLabCut can also be used for color videos. There is no inherent limitation to the cameras that can be used to collect videos that can subsequently be analyzed with our toolbox. Please see http:www.mousemotorlab. orgdeeplabcut for more example behaviors (including examples of color videos).', 'Mouse odor trail-tracking. The trail-tracking behavior is part of an investigation into odor-guided navigation wherein one or more wild-type (C57BL6J) male mice run on a paper spool following odor trails. These experiments were carried out in the laboratory of Venkatesh Murthy at Harvard University and will be published elsewhere. For trail-tracking, we extracted 1,080 random, distinct frames from multiple experimental sessions observing 7 different mice. Data were recorded at 30 Hz by two different cameras: the 640  480 pixels images were acquired with a Point Grey Firefly FMVU-03MTM-CS and the 1,700  1,200 pixel images with a Point Grey Grasshopper 3 4.1MP Mono USB3 Vision, CMOSIS CMV4000-3E12. The latter images are prohibitively large to process without downsampling, and therefore we cropped around mice to generate images that were approximately 800  800 pixels. One human annotator was instructed to localize the snout, the tip of the left and right ear and the base of the tail in the example images on two different occasions (using Fiji 49 ), which generated two distinct label sets ( 1 month apart to reduce memory bias; see Fig. 1).', 'Mouse reach and pull joystick task. Experimental procedures for the training of the joystick behavior and the construction of the behavioral set-up can be found in Mathis et al. 38 . In brief, head-fixed mice were trained to reach, grab and pull a joystick for a liquid reward. To generate a traintest set of images, we labeled 159 frames at 13 locations: 3 points per digit-the digit tip, the joint in the middle and the base of the digit (which roughly correspond to the proximal interphalangeal joint and the metacarpophalangeal joint, respectively)-as well as the base of the hand (wrist). The data were collected across 5 different mice (C57BL6J, male and female) and were recorded at 2,048  1,088 resolution with a frame rate of 100-320 Hz. For tracking the digits, we used the supplied toolbox code to crop the data to extract only regions of interest containing the movement of the forelimb to limit the size of the input image to the network.', 'All surgical and experimental procedures for mice were in accordance with the National Institutes of Health Guide for the Care and Use of Laboratory Animals and approved by the Harvard Institutional Animal Care and Use Committee.', 'Drosophila egg-laying behavior. Experiments were carried out in the laboratory of Richard Axel at Columbia University and will be published elsewhere. In brief, egg-laying behavior in female Drosophila (Canton-S strain) was observed in customdesigned 3D-printed chambers (Protolabs). Individual chambers were 4.1 mm deep and tapered from top to bottom, with the top dimensions 7.3 mm  5.8 mm and the bottom dimensions 6.7 mm  4.3 mm. One side of the chamber opened to a reservoir within which 1 agar was poured and allowed to set. Small acrylic windows were slid into place along grooves at the top and bottom to enclose the fly within the chamber and to allow viewing. The chamber was illuminated by a 2-inch off-axis ring light (Metaphase) and video recording was performed from above the chamber using an infrared-sensitive CMOS camera (Basler) with a 0.5 telecentric lens (Edmund Optics) at 20 Hz (682  540 pixels). We identified 12 distinct points of interest to quantify the behavior of interest on the body of the fly. One human annotator manually extracted 589 distinct and informative frames from six different animals, labeling only those features that were visible within a given frame. The 12 points comprise 4 points on the head (the dorsal tip of each compound eye, the ocellus and the tip of the proboscis), the posterior tip of the scutellum on the thorax, the joint between the femur and tibia on each metathoracic leg, the abdominal stripes on the four most posterior abdominal segments (A3-A6) and the ovipositor.', 'Labeled dataset set selection. No statistical methods were used to predetermine sample sizes for labeled frames, but our sample sizes are similar to those reported in previous publications. The labelers were blinded to whether the frames would be assigned to training or test datasets (as the frames were randomized across splits). For each replicate (i.e., split of the dataset), frames were randomly assigned to the test or training set. No data or experimental animals (mice or Drosophila) were excluded from the study.', 'Deep feature detector architecture. We employ strong body part detectors, which are part of state-of-the art algorithms for human pose estimation called DeeperCut 26,27,29 . Those part detectors build on state-of-the-art object recognition architectures, namely extremely deep residual networks (ResNet) 24 . Specifically, we use a variant of the ResNet with 50 layers, which achieved outstanding performance in object recognition competitions 24 . In the DeeperCut implementation, the ResNets were adapted to represent the images with higher spatial resolution, and the softmax layer used in the original architecture after the conv5 bank (as would be appropriate for object classification) was replaced by deconvolutional layers that produce a scalar field of activation values corresponding to regions in original image. This output is also connected to the conv3 bank to make use of finer features generated earlier in the ResNet architecture 27 . For each body part, there is a corresponding output layer whose activity represents probability score-maps . These score-maps represent the probability that a body part is at a particular pixel 26,27 . During training, a score-map with positive label 1 (unity probability) is generated for all locations up to ϵ pixels away from the ground truth per body part (distance variable). The ResNet architecture used to generate features is initialized with weights trained on ImageNet 24 , and the cross-entropy loss between the predicted score-map and the ground-truth score-map is minimized by stochastic gradient descent 27 . Around 500,000 training steps were enough for convergence in the presented cases, and training takes up to 24 h on a GPU (NVIDIA GTX 1080 Ti; note that typically the loss starts to slowly decay early in training; see Fig. 2b). We used a batch size of 1, which allows us to have images of different sizes, decreased the learning rate over training and performed data augmentation during training by rescaling the images (as in DeeperCut, but we used a range of 50 to 150). We also tested further data augmentation by additionally training with 7 rotated frames per training image (rotation group-angles independently and uniformly sampled (uid) from -8,8 degrees) as well as 9 rotated and 14 partial images per training images (rotation and translation group-angles uid from -10,10 degrees, as well as uid subimages amounting to relative shifts). Unless otherwise noted, we used a distance variable ϵ  17 (pixel radius) and scale factor 0.8 (which affects the ratio of the input image to output score-map). We cross-validated the choice of ϵ for a higher resolution output (scale factor  1) and found that the test performance was not improved when varying ϵ widely, but the rate of performance improvement was strongly decreased for small ϵ (Supplementary Fig. 2). We also evaluated deeper networks with 101 layers, ResNet-101, as well as ResNet-101ws (with intermediate supervision, Supplementary Fig. 2b); more technical details can be found in Insafutdinov et al. 27 .', 'Evaluation and error measures. The trained network can be used to predict body part locations. At any state of training, the network can be presented with novel frames, for which the prediction of the location of a particular body part is given by the peak of the corresponding score-map. This estimate is further refined on the basis of learned correspondences between the score-map grid and ground truth joint positions 26,27,29 . In the case of multiple mice, the local maxima of the scoremap are extracted as predictions of the body part locations (Fig. 4).', 'As discussed in the main text, a user can continue to fine-tune the network for increasing generalization to large datasets to reduce errors. One can use features of the score-maps such as the amplitude and width, or heuristics such as the continuity of body part trajectories, to identify images for which the decoder might make large errors. Images with insufficient automatic labeling performance that are identified in this way can then be manually labeled to increase the training set and iteratively improve the feature detectors.', 'To compare between datasets generated by the human scorer, as well as with or between model-generated labels, we used the Euclidean distance (root mean square error, RMSE) calculated pairwise per body part. Depending on the context, this metric is either shown for a specific body part, averaged over all body parts, or averaged over a set of images. To quantify the error across learning, we stored snapshots of the weights in TensorFlow 50 (usually every 50,000 iterations) and evaluated the RMSE for predictions generated by these frozen networks post hoc. Note that the RMSE is not the loss function minimized during training. However, the RMSE is the relevant performance metric for assessing labeling precision in pixels.', 'The RMSE between the first and second annotation is referred to as human variability. In figures we also depict the 95 confidence interval for this RMSE, whose limits are given as mean  1.96 times the s.e.m. (Figs 2c,d,f, 4 and 5 and Supplementary Fig. 2a-d). Depending on the figure, the RMSE is averaged over all or just a subset of body parts. For the Drosophila and the mouse hand data, we report the average test RMSE for all body parts with likelihood larger than 10.', 'In Fig. 7 we extracted cropped images of the hand from full frames (n  1,139) by centering it using the predicted wrist position. We then performed dimensionality reduction by t-SNE embedding of those images 51 and randomly selected certain sufficiently distant points to illustrate the corresponding hand postures.', 'Reporting Summary. Further information on experimental design is available in the Nature Research Reporting Summary linked to this article.']\"}]}\n",
      "\u001b[00m\n",
      "\u001b[92m14:06:04 - LiteLLM:INFO\u001b[0m: utils.py:2896 - \n",
      "LiteLLM completion() model= openai/gpt-4o-2024-11-20; provider = openrouter\n",
      "2025-04-04 14:06:04,630 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= openai/gpt-4o-2024-11-20; provider = openrouter\n",
      "2025-04-04 14:06:04,883 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 401 Unauthorized\"\n",
      "\n",
      "\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "2025-04-04 14:06:04,968 - root - ERROR - LiteLLM call failed: litellm.AuthenticationError: AuthenticationError: OpenrouterException - {\"error\":{\"message\":\"No auth credentials found\",\"code\":401}}\n",
      "\u001b[91m Error during LLM call: litellm.AuthenticationError: AuthenticationError: OpenrouterException - {\"error\":{\"message\":\"No auth credentials found\",\"code\":401}}\u001b[00m\n",
      "\u001b[91m An unknown error occurred. Please check the details below.\u001b[00m\n",
      "\u001b[91m Error details: litellm.AuthenticationError: AuthenticationError: OpenrouterException - {\"error\":{\"message\":\"No auth credentials found\",\"code\":401}}\u001b[00m\n",
      "2025-04-04 14:06:04,969 - structsense.app - ERROR - Step execution failed: litellm.AuthenticationError: AuthenticationError: OpenrouterException - {\"error\":{\"message\":\"No auth credentials found\",\"code\":401}}\n",
      "2025-04-04 14:06:04,969 - structsense.app - ERROR - Flow execution failed: Step execution failed: litellm.AuthenticationError: AuthenticationError: OpenrouterException - {\"error\":{\"message\":\"No auth credentials found\",\"code\":401}}\n",
      "\u001b[1;34m🌊 Flow: \u001b[0m\u001b[34mStructSenseFlow\u001b[0m\n",
      "\u001b[37m    ID: \u001b[0m\u001b[34m1122876d-91d0-4213-9bad-6660a729cf99\u001b[0m\n",
      "├── \u001b[31m❌ Flow Step Failed\u001b[0m\n",
      "└── \u001b[1;31m❌ Failed:\u001b[0m\u001b[1;31m kickoff_flow\u001b[0m\n",
      "\n",
      "2025-04-04 14:06:04,971 - structsense.app - ERROR - Flow execution failed: Flow execution failed: Step execution failed: litellm.AuthenticationError: AuthenticationError: OpenrouterException - {\"error\":{\"message\":\"No auth credentials found\",\"code\":401}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/litellm/llms/openai_like/chat/handler.py\", line 372, in completion\n",
      "    response = client.post(\n",
      "               ^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 553, in post\n",
      "    raise e\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 534, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '401 Unauthorized' for url 'https://openrouter.ai/api/v1/chat/completions'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/litellm/main.py\", line 2237, in completion\n",
      "    response = openai_like_chat_completion.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/litellm/llms/openai_like/chat/handler.py\", line 378, in completion\n",
      "    raise OpenAILikeError(\n",
      "litellm.llms.openai_like.common_utils.OpenAILikeError: {\"error\":{\"message\":\"No auth credentials found\",\"code\":401}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/structsense/app.py\", line 247, in run_step\n",
      "    result = crew.kickoff(inputs=inputs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/crew.py\", line 640, in kickoff\n",
      "    result = self._run_sequential_process()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/crew.py\", line 752, in _run_sequential_process\n",
      "    return self._execute_tasks(self.tasks)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/crew.py\", line 850, in _execute_tasks\n",
      "    task_output = task.execute_sync(\n",
      "                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/task.py\", line 310, in execute_sync\n",
      "    return self._execute_core(agent, context, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/task.py\", line 454, in _execute_core\n",
      "    raise e  # Re-raise the exception after emitting the event\n",
      "    ^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/task.py\", line 374, in _execute_core\n",
      "    result = agent.execute_task(\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/agent.py\", line 266, in execute_task\n",
      "    raise e\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/agent.py\", line 247, in execute_task\n",
      "    result = self.agent_executor.invoke(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/agents/crew_agent_executor.py\", line 119, in invoke\n",
      "    raise e\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/agents/crew_agent_executor.py\", line 108, in invoke\n",
      "    formatted_answer = self._invoke_loop()\n",
      "                       ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/agents/crew_agent_executor.py\", line 166, in _invoke_loop\n",
      "    raise e\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/agents/crew_agent_executor.py\", line 146, in _invoke_loop\n",
      "    answer = self._get_llm_response()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/agents/crew_agent_executor.py\", line 216, in _get_llm_response\n",
      "    raise e\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/agents/crew_agent_executor.py\", line 207, in _get_llm_response\n",
      "    answer = self.llm.call(\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/llm.py\", line 739, in call\n",
      "    return self._handle_non_streaming_response(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/llm.py\", line 575, in _handle_non_streaming_response\n",
      "    response = litellm.completion(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/litellm/utils.py\", line 1154, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/litellm/utils.py\", line 1032, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/litellm/main.py\", line 3068, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2201, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2073, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenrouterException - {\"error\":{\"message\":\"No auth credentials found\",\"code\":401}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/structsense/app.py\", line 193, in kickoff_flow\n",
      "    result = self.run_step(step)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/structsense/app.py\", line 270, in run_step\n",
      "    raise FlowExecutionError(f\"Step execution failed: {str(e)}\")\n",
      "structsense.app.FlowExecutionError: Step execution failed: litellm.AuthenticationError: AuthenticationError: OpenrouterException - {\"error\":{\"message\":\"No auth credentials found\",\"code\":401}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/bin/structsense-cli\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "             ^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/click/core.py\", line 1161, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/click/core.py\", line 1082, in main\n",
      "    rv = self.invoke(ctx)\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/click/core.py\", line 1697, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/click/core.py\", line 1443, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/click/core.py\", line 788, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/structsense/cli.py\", line 69, in extract\n",
      "    result = kickoff(\n",
      "             ^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/structsense/app.py\", line 352, in kickoff\n",
      "    final_response =  flow.kickoff()\n",
      "                      ^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/flow/flow.py\", line 722, in kickoff\n",
      "    return asyncio.run(run_flow())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/asyncio/runners.py\", line 190, in run\n",
      "    return runner.run(main)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/asyncio/runners.py\", line 118, in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/flow/flow.py\", line 720, in run_flow\n",
      "    return await self.kickoff_async(inputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/flow/flow.py\", line 787, in kickoff_async\n",
      "    await asyncio.gather(*tasks)\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/flow/flow.py\", line 820, in _execute_start_method\n",
      "    result = await self._execute_method(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/flow/flow.py\", line 876, in _execute_method\n",
      "    raise e\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/crewai/flow/flow.py\", line 846, in _execute_method\n",
      "    else method(*args, **kwargs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/structsense/app.py\", line 201, in kickoff_flow\n",
      "    raise FlowExecutionError(f\"Flow execution failed: {str(e)}\")\n",
      "structsense.app.FlowExecutionError: Flow execution failed: Step execution failed: litellm.AuthenticationError: AuthenticationError: OpenrouterException - {\"error\":{\"message\":\"No auth credentials found\",\"code\":401}}\n",
      "/Users/brukewossenseged/structsense/.conda/lib/python3.11/site-packages/urllib3/poolmanager.py:274: ResourceWarning: unclosed <ssl.SSLSocket fd=7, family=2, type=1, proto=0, laddr=('10.31.173.118', 64472), raddr=('209.38.174.229', 4319)>\n",
      "  self.pools.clear()\n"
     ]
    }
   ],
   "source": [
    "!structsense-cli extract --agentconfig bbqs_resource_config/bbqs_agent.yaml --taskconfig bbqs_resource_config/bbqs_task.yaml --embedderconfig bbqs_resource_config/embedding.yaml --flowconfig bbqs_resource_config/flow_bbqs.yaml --source /Users/brukewossenseged/Documents/dlc.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
